{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqWXeTH+JHiK6A1xtvI8HN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1lMh5KI0Bcty","executionInfo":{"status":"ok","timestamp":1682973610948,"user_tz":420,"elapsed":62726,"user":{"displayName":"Tam Huynh","userId":"08201832801118655242"}},"outputId":"194f4869-87a4-4ee1-a960-93e9cf64c668"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers[sentencepiece]\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Collecting aiohttp (from datasets)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Collecting responses<0.19 (from datasets)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[sentencepiece])\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n","  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting protobuf<=3.20.2 (from transformers[sentencepiece])\n","  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: tokenizers, sentencepiece, xxhash, protobuf, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets, evaluate\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n","tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 protobuf-3.20.2 responses-0.18.0 sentencepiece-0.1.98 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0 yarl-1.9.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting accelerate\n","  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (16.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.18.0\n"]}],"source":["!pip install datasets evaluate transformers[sentencepiece]\n","!pip install accelerate"]},{"cell_type":"code","source":["import pandas as pd\n","import json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns \n","# from bs4 import BeautifulSoup\n","\n","from datasets import DatasetDict, Dataset, load_dataset\n","from sklearn.model_selection import train_test_split\n","from pydrive.auth import GoogleAuth\n","from google.colab import drive\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# We can see all columns in df.head() / and .tail()\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', 1000) \n","pd.set_option('display.max_colwidth', None)"],"metadata":{"id":"__pVpvMzBh27","executionInfo":{"status":"ok","timestamp":1682973615881,"user_tz":420,"elapsed":4939,"user":{"displayName":"Tam Huynh","userId":"08201832801118655242"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","#original RecipeQA\n","train_dataid = '1c206RpN1YCecrL6Hvjl52a3R16Lf3cO1'\n","val_dataid = '1X0RXXyTaaSUqJgbiuNzTJkB0ryZhkdwo'\n","test_dataid = '1Xp7zTJOVV3ZeEaqEcfl1MziJY7JcxoJh'\n","\n","download = drive.CreateFile({'id': train_dataid})\n","download.GetContentFile('train_og.json')\n","\n","download = drive.CreateFile({'id': val_dataid})\n","download.GetContentFile('val_og.json')\n","\n","download = drive.CreateFile({'id': test_dataid})\n","download.GetContentFile('test_og.json')"],"metadata":{"id":"gn7qR-mXBi-8","executionInfo":{"status":"ok","timestamp":1682973641784,"user_tz":420,"elapsed":25905,"user":{"displayName":"Tam Huynh","userId":"08201832801118655242"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Convert original data to desired format for QA"],"metadata":{"id":"HlllfWUMBlLa"}},{"cell_type":"code","source":["def make_text_visual_df(dataset_type):\n","  f = open('{dataset}_og.json'.format(dataset=dataset_type), 'r')\n","\n","  read_data = json.loads(f.read())\n","  textual_json = [x for x in read_data['data'] if x['task'] == 'textual_cloze']\n","  visual_json = [x for x in read_data['data'] if x['task'] == 'visual_coherence']\n","\n","  textual_df = pd.DataFrame(textual_json)\n","  textual_keep_col = ['recipe_id', 'context', 'choice_list', 'answer', 'question']\n","  textual_df = textual_df[textual_keep_col]\n","\n","  visual_df = pd.DataFrame(visual_json)\n","  visual_keep_col = ['recipe_id', 'context']\n","  visual_df = visual_df[visual_keep_col]\n","\n","  return textual_df, visual_df\n","\n","def combine_all_steps(row): \n","  all_steps = []\n","  num_steps = len(row.context)\n","  for step in range(num_steps):\n","    all_steps.append(\"Step \" + str(step+1) + \": \" + row.context[step]['title'])\n","  return all_steps\n","\n","def combine_text_visual_df(dataset_type):\n","  textual_df, visual_df = make_text_visual_df(dataset_type) #replace w dataset type\n","  visual_df['all_steps'] = visual_df.apply(lambda row: combine_all_steps(row), axis=1)\n","  train_data = pd.merge(textual_df, visual_df, how='inner', on=['recipe_id'])\n","  train_data.rename(columns={'context_x': 'context'}, inplace=True)\n","  train_data = train_data[['recipe_id', 'context', 'choice_list', 'answer', 'question', 'all_steps']]\n","  train_data.question = train_data.question.apply(lambda x: [i.replace('@placeholder', '_') if i == '@placeholder' else i for i in x])\n","  return train_data\n","\n","def combine_body_with_step(row):\n","  full_instruction = \"\"\n","  context = row[\"context\"]\n","  steps = row[\"all_steps\"]\n","  for step in range(len(steps)):\n","    full_instruction += str(steps[step]) + \": \" + context[step]['body'] + \". \"\n","  return full_instruction[0:-1]\n","\n","def generate_questions(row):\n","  create_question = \"\"\n","  given_question = row['question']\n","  target_index = given_question.index('_')\n","  #two ways to create question... not sure if one is better than another\n","  #feel free to suggest ways to create questions :)\n","  if target_index == 0 or target_index == 2:\n","    create_question = \"What is the step before \" + given_question[target_index + 1] + \"?\"\n","  else:\n","    create_question = \"What is the step after \" + given_question[target_index-1] + \"?\"\n","  # if target_index == 0:\n","  #   create_question = \"What is the step before \" + given_question[target_index+1] + \"?\"\n","  # elif target_index == 3:\n","  #   create_question = \"What is the step after \" + given_question[target_index-1] + \"?\"\n","  # else:\n","  #   create_question = \"What is the step after \" + given_question[target_index-1] + \" and before \" + given_question[target_index+1] + \"?\"\n","  return create_question\n","\n","def generate_answer_and_index(row):\n","  actual_answer = {}\n","  answer = row[\"choice_list\"][row.answer]\n","  actual_answer[\"text\"] = [answer]\n","  full_instruction = row.full_instruction\n","\n","  actual_answer[\"answer_start\"] = [full_instruction.find(answer)]\n","  return actual_answer\n","\n","def make_final_data(dataset_type):\n","  combine_data = combine_text_visual_df(dataset_type) #replace datasettype\n","  combine_data['full_instruction'] = combine_data.apply(lambda row: combine_body_with_step(row), axis=1)\n","  combine_data['new_question'] = combine_data.apply(lambda row: generate_questions(row), axis=1)\n","  combine_data['actual_answer'] = combine_data.apply(lambda row: generate_answer_and_index(row), axis=1)\n","  dup_check = combine_data[['recipe_id', 'full_instruction', 'new_question', 'answer']]\n","  combine_data = combine_data[dup_check.duplicated() == False].reset_index(drop=True)\n","  final_data = combine_data[['recipe_id', 'full_instruction', 'new_question', 'actual_answer']].reset_index()\n","  final_data.rename(columns={'index':'id', 'recipe_id':'title', 'full_instruction':'context', 'new_question':'question', 'actual_answer':'answers'}, inplace=True)\n","  return final_data"],"metadata":{"id":"Xop-xFpUBoPZ","executionInfo":{"status":"ok","timestamp":1682973641784,"user_tz":420,"elapsed":16,"user":{"displayName":"Tam Huynh","userId":"08201832801118655242"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["train_df = make_final_data('train')\n","val_df = make_final_data('val')\n","test_df = make_final_data('test')"],"metadata":{"id":"MZoBMfGtBrMG","executionInfo":{"status":"ok","timestamp":1682973650288,"user_tz":420,"elapsed":8519,"user":{"displayName":"Tam Huynh","userId":"08201832801118655242"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["train = Dataset.from_pandas(train_df)\n","val = Dataset.from_pandas(val_df)\n","test = Dataset.from_pandas(test_df) \n","\n","full_dataset = DatasetDict({'train': train, 'val': val, 'test': test})"],"metadata":{"id":"NX9B3eqaBsgz","executionInfo":{"status":"ok","timestamp":1682973650433,"user_tz":420,"elapsed":163,"user":{"displayName":"Tam Huynh","userId":"08201832801118655242"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["full_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nkdh0lryKDkc","executionInfo":{"status":"ok","timestamp":1682975509403,"user_tz":420,"elapsed":199,"user":{"displayName":"Tam Huynh","userId":"08201832801118655242"}},"outputId":"c8b148c4-7d92-438a-d74c-f5316f664adb"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'title', 'context', 'question', 'answers'],\n","        num_rows: 5597\n","    })\n","    val: Dataset({\n","        features: ['id', 'title', 'context', 'question', 'answers'],\n","        num_rows: 645\n","    })\n","    test: Dataset({\n","        features: ['id', 'title', 'context', 'question', 'answers'],\n","        num_rows: 686\n","    })\n","})"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["#everyone can make a copy and take it from here to play around with qa modeling"],"metadata":{"id":"-KHURkePBtwX"},"execution_count":null,"outputs":[]}]}